{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "style-transfer-test-with-yolo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5a388f3579c14240a8fe0baa4d1d3470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b17fa1bd97ae4da0b18cd9d1034f7603",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7ee59baff9c5498f9cf7088f5edfab87",
              "IPY_MODEL_603037b89b8b4714953c9b772b14117c"
            ]
          }
        },
        "b17fa1bd97ae4da0b18cd9d1034f7603": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ee59baff9c5498f9cf7088f5edfab87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f695337c911c43f19c1cf4ed1a561db2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e2b812e0fccc49c881912b6ca5eca355"
          }
        },
        "603037b89b8b4714953c9b772b14117c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2807e02861f44f8d81e8cbdbcc06605a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/0 [00:47&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3587b5eaad2d4a76a531b4cd80979dbe"
          }
        },
        "f695337c911c43f19c1cf4ed1a561db2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e2b812e0fccc49c881912b6ca5eca355": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2807e02861f44f8d81e8cbdbcc06605a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3587b5eaad2d4a76a531b4cd80979dbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca6ca7cdbb9b4a7ca6ede097e1f8d520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_79082d4a49894187991aeeeb7b23f3d3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8182ccacb5e24aaa907b3ba2b931c765",
              "IPY_MODEL_3fa60069055c44eda55004eb0a2265db"
            ]
          }
        },
        "79082d4a49894187991aeeeb7b23f3d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8182ccacb5e24aaa907b3ba2b931c765": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_41456b7c47594d54809e813f66585ff5",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 12927,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 12927,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2368234822a3434e8be9b380f47c3b50"
          }
        },
        "3fa60069055c44eda55004eb0a2265db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0471f46f967f428d935aec2c1c373e5b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 12927/12927 [00:43&lt;00:00, 295.98it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7564f2f1845e47e7a1d531804209755d"
          }
        },
        "41456b7c47594d54809e813f66585ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2368234822a3434e8be9b380f47c3b50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0471f46f967f428d935aec2c1c373e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7564f2f1845e47e7a1d531804209755d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d85b90e409474ab5ba2e9dd620f2823b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_18869c3ae36d468585be82a8ae817606",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1acfe9561e5d4717930b994fab6848f2",
              "IPY_MODEL_cf7ad306a39a48698f8e7f5b9572f79b"
            ]
          }
        },
        "18869c3ae36d468585be82a8ae817606": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1acfe9561e5d4717930b994fab6848f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7b249483adc64165ab84fb9aceb4a3d2",
            "_dom_classes": [],
            "description": "  3%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1194,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 35,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c98eb70c38246e7ac54f566b0a4eef9"
          }
        },
        "cf7ad306a39a48698f8e7f5b9572f79b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a456698fdff54b1aa33e6a26e08cb473",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 35/1194 [01:06&lt;30:36,  1.58s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_45161e1989954bbeb6ed3efb6f33390b"
          }
        },
        "7b249483adc64165ab84fb9aceb4a3d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c98eb70c38246e7ac54f566b0a4eef9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a456698fdff54b1aa33e6a26e08cb473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "45161e1989954bbeb6ed3efb6f33390b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZg60laaY3Ya",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "82a6eeff-7138-4aca-d291-39e42e5481ed"
      },
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi\n",
        "!cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "Sun Aug 16 13:35:27 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    30W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "#define CUDNN_MAJOR 7\n",
            "#define CUDNN_MINOR 6\n",
            "#define CUDNN_PATCHLEVEL 5\n",
            "--\n",
            "#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n",
            "\n",
            "#include \"driver_types.h\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nhfag33ZSep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb046c6b-2b31-4ccb-85fd-ad0481a217c5"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.6.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW_6MKQgeo5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215,
          "referenced_widgets": [
            "5a388f3579c14240a8fe0baa4d1d3470",
            "b17fa1bd97ae4da0b18cd9d1034f7603",
            "7ee59baff9c5498f9cf7088f5edfab87",
            "603037b89b8b4714953c9b772b14117c",
            "f695337c911c43f19c1cf4ed1a561db2",
            "e2b812e0fccc49c881912b6ca5eca355",
            "2807e02861f44f8d81e8cbdbcc06605a",
            "3587b5eaad2d4a76a531b4cd80979dbe",
            "ca6ca7cdbb9b4a7ca6ede097e1f8d520",
            "79082d4a49894187991aeeeb7b23f3d3",
            "8182ccacb5e24aaa907b3ba2b931c765",
            "3fa60069055c44eda55004eb0a2265db",
            "41456b7c47594d54809e813f66585ff5",
            "2368234822a3434e8be9b380f47c3b50",
            "0471f46f967f428d935aec2c1c373e5b",
            "7564f2f1845e47e7a1d531804209755d"
          ]
        },
        "outputId": "4547a04a-2de5-4215-84a4-035d0f66bd38"
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "path_0 = \"sentiment.dev.0\"\n",
        "path_1 = \"sentiment.dev.1\"\n",
        "\n",
        "def equalize_seq_num(path_0, path_1):\n",
        "    \"\"\"\n",
        "    TorchText Bucket Iterator 내부에서 batch 개수를 똑같이 맞출 수가 없어서,\n",
        "    일단 pandas를 이용해서 sequence의 개수를 동일하게 먼저 만들어 준 후에,\n",
        "    파일을 저장해서 사용한다. 태완님이 좋은 코드를 짜주셨기를 바라면서...\n",
        "    \"\"\"\n",
        "    data0 = pd.read_table(path_0, header=None)\n",
        "    data1 = pd.read_table(path_1, header=None)\n",
        "\n",
        "    if len(data0) > len(data1):\n",
        "        larger, smaller = data0, data1\n",
        "    else:\n",
        "        larger, smaller = data1, data0\n",
        "\n",
        "    print(\"smaller: \" + str(len(smaller)))\n",
        "    print(\"larger: \" + str(len(larger)))\n",
        "\n",
        "    repeat_num = int( (len(larger) - len(smaller)) / len(smaller))\n",
        "    print(repeat_num)\n",
        "    remain_num = (len(larger)-len(smaller)) % len(smaller)\n",
        "    print(remain_num)\n",
        "    \n",
        "\n",
        "    for i in tqdm(range(repeat_num)):\n",
        "        smaller = smaller.append(smaller, ignore_index=True)\n",
        "    \n",
        "    len_small = len(smaller)\n",
        "    for i in tqdm(range(remain_num)):\n",
        "        smaller = smaller.append(smaller.loc[i%len_small], ignore_index=True)\n",
        "\n",
        "\n",
        "    #for i in tqdm(range(len(larger) - len(smaller))):\n",
        "    #    smaller = smaller.append(smaller.loc[i%len_small], ignore_index=True)\n",
        "    #    i+=1\n",
        "    print(\"data length: {}\".format(len(smaller)))\n",
        "    assert len(larger)==len(smaller)\n",
        "\n",
        "    if len(data0) > len(data1):\n",
        "        larger.to_csv(path_0, header=None, index=None, sep=' ')\n",
        "        smaller.to_csv(path_1, header=None, index=None, sep=' ')\n",
        "    else:\n",
        "        larger.to_csv(path_1, header=None, index=None, sep=' ')\n",
        "        smaller.to_csv(path_0, header=None, index=None, sep=' ')       \n",
        "    \n",
        "    \n",
        "equalize_seq_num(path_0, path_1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "smaller: 25278\n",
            "larger: 38205\n",
            "0\n",
            "12927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a388f3579c14240a8fe0baa4d1d3470",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca6ca7cdbb9b4a7ca6ede097e1f8d520",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=12927.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "data length: 38205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Prg9b248oASF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b732398-f922-408d-8efd-5a3779ea767d"
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "data0 = pd.read_table(path_0, header=None)\n",
        "data1 = pd.read_table(path_1, header=None)\n",
        "\n",
        "print(len(data0), len(data1), sep='\\t')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "38205\t38205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu_f8U5Pws2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext.data as data\n",
        "\n",
        "class BucketIterator_complete_last(data.BucketIterator): # return last batch of batch_size\n",
        "    def batch(data, batch_size, batch_size_fn=None):\n",
        "        \"\"\"Yield elements from data in chunks of batch_size.\"\"\"\n",
        "        if batch_size_fn is None:\n",
        "            def batch_size_fn(new, count, sofar):\n",
        "                return count\n",
        "        minibatch, size_so_far = [], 0\n",
        "        for ex in data:\n",
        "            minibatch.append(ex)\n",
        "            size_so_far = batch_size_fn(ex, len(minibatch), size_so_far)\n",
        "            if size_so_far == batch_size:\n",
        "                yield minibatch\n",
        "                minibatch, size_so_far = [], 0\n",
        "            elif size_so_far > batch_size:\n",
        "                yield minibatch[:-1]\n",
        "                minibatch, size_so_far = minibatch[-1:], batch_size_fn(ex, 1, 0)\n",
        "\n",
        "        if minibatch and size_so_far < batch_size:\n",
        "            for ex in data[:batch_size - size_so_far]:\n",
        "                minibatch.append(ex)\n",
        "            yield minibatch\n",
        "        if minibatch:\n",
        "            yield minibatch\n",
        "\n",
        "\n",
        "def preprocessing(text):\n",
        "    if text[-1]==\".\":\n",
        "        text.pop(-1)\n",
        "    return text\n",
        "\n",
        "# Init Field\n",
        "\n",
        "TEXT = data.Field(\n",
        "    tokenize='spacy',\n",
        "    preprocessing=preprocessing,\n",
        "    init_token = '<sos>',\n",
        "    eos_token = '<eos>',\n",
        "    include_lengths=True,\n",
        "    lower = True\n",
        ")\n",
        "\n",
        "# Init TabularDataset\n",
        "dataset0 = data.TabularDataset(\n",
        "    path='sentiment.dev.0',\n",
        "    format='tsv',\n",
        "    fields=[('text0',TEXT)],\n",
        ")\n",
        "dataset1 = data.TabularDataset(\n",
        "    path='sentiment.dev.1',\n",
        "    format='tsv',\n",
        "    fields=[('text1',TEXT)]\n",
        ")\n",
        "\n",
        "## Build Vocab\n",
        "\n",
        "TEXT.build_vocab(dataset0, dataset1, min_freq=3, vectors=\"glove.6B.100d\")\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "## make iterator\n",
        "iterator_0=BucketIterator_complete_last(\n",
        "    dataset0, \n",
        "    batch_size=batch_size,\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x : len(x.text0),\n",
        ")\n",
        "\n",
        "iterator_1=BucketIterator_complete_last(\n",
        "    dataset1, \n",
        "    batch_size=batch_size,\n",
        "    sort_within_batch=True,\n",
        "    sort_key=lambda x : len(x.text1)\n",
        ")"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux49VET-NvV7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "e2fabc78-4298-4ba6-afa5-1235a6a6c0f0"
      },
      "source": [
        "for i in range(10):\n",
        "    print(dataset0[i].text0)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ok', 'never', 'going', 'back', 'to', 'this', 'place', 'again']\n",
            "['easter', 'day', 'nothing', 'open', ',', 'heard', 'about', 'this', 'place', 'figured', 'it', 'would', 'ok']\n",
            "['the', 'host', 'that', 'walked', 'us', 'to', 'the', 'table', 'and', 'left', 'without', 'a', 'word']\n",
            "['it', 'just', 'gets', 'worse']\n",
            "['the', 'food', 'tasted', 'awful']\n",
            "['no', 'sign', 'of', 'the', 'manager']\n",
            "['the', 'last', 'couple', 'years', 'this', 'place', 'has', 'been', 'going', 'down', 'hill']\n",
            "['last', 'night', 'however', 'it', 'was', 'way', 'to', 'thick', 'and', 'tasteless']\n",
            "['it', 'smelled', 'like', 'rotten', 'urine']\n",
            "['i', 'am', 'not', 'exaggerating']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9O5ck_a1aRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding, dim_y, dim_z, dropout):\n",
        "        \"\"\"\n",
        "        Required parameters:\n",
        "            batch_size:\n",
        "\n",
        "            dim_y: \n",
        "            dim_z: \n",
        "            embed_dim:\n",
        "        \n",
        "        구성요소:\n",
        "            word_embedding을 포함하지 않는 이유: 하나의 encoder를 유지하기 위해(test code를 eng - kor 데이터로 짜다 보니까 이렇게 되었음..)\n",
        "            yolo 데이터로 코드 바꿀 예정\n",
        "            Fully connected Layer\n",
        "            unidirectional GRU\n",
        "            \n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear( 1, dim_y) # output: (batch_size, dim_y)\n",
        "        self.init_z = torch.zeros(dim_z)\n",
        "        self.embed = embedding\n",
        "\n",
        "        self.rnn = nn.GRU(self.embed.embedding_dim, dim_y + dim_z, num_layers=1, dropout=dropout)\n",
        "        self.dim_y = dim_y\n",
        "    \n",
        "    def forward(self, labels, src, src_len ):\n",
        "        labels = labels.unsqueeze(-1) # (batch_size, 1), 엔코더 밖에서 해줘도 괜찮을 듯\n",
        "        src = self.embed(src)\n",
        "        packed_embed = nn.utils.rnn.pack_padded_sequence(src, src_len) # input input to rnn\n",
        "        \n",
        "        # initial hidden state of the encoder: concat ( y, z)   \n",
        "        \n",
        "        init_z = self.init_z.repeat(src.shape[1], 1) # [ batch size: src.shape[1] , dim_z ]\n",
        "        init_hidden = torch.cat((self.fc(labels), init_z), -1) \n",
        "\n",
        "        _, hidden = self.rnn(packed_embed, init_hidden.unsqueeze(0))\n",
        "        # hidden : hidden_state of the final time step\n",
        "        hidden = hidden.squeeze(0)\n",
        "        z = hidden[:, self.dim_y:]\n",
        "        return z\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "        Required parameters:\n",
        "            embedding: nn.Embedding()\n",
        "            embed_dim: dimension of embedding (repetition, could be erased if necessary)\n",
        "            dim_y: .\n",
        "            dim_z: .\n",
        "            dropout: refer to the paper\n",
        "            temperature: refer to the paper\n",
        "            idx_sos: TEXT.vocab['<sos>'] set to torch.tensor([2], dtype=int) as default\n",
        "        \n",
        "        Components:\n",
        "            Word Embedding : generator의 경우에는 여러개가 있어도 괜찮을 듯?\n",
        "            Unidirectional GRU\n",
        "            Fully connected Layer (prediction)\n",
        "    \"\"\"\n",
        "    def __init__(self, embeddings, dim_y, dim_z, dropout, temperature, idx_sos=2):\n",
        "        #TODO: self.Generator 생성시에 embedding을 넣어주면 embed_dim을 넣어줄 필요가 없다. generator 개수가 여러개여도 되는지 논의해보고 결정\n",
        "        \n",
        "        super().__init__() \n",
        "        self.gamma = temperature\n",
        "        self.dim_h = dim_y + dim_z\n",
        "\n",
        "        self.embed = embeddings # type(embeddings) = nn.Embedding\n",
        "        self.index_sos = torch.tensor([idx_sos],dtype=int) # to feed <sos> when generating a transfered text\n",
        "\n",
        "        self.fc = nn.Linear(1, dim_y)\n",
        "        # The hidden state's dimension: dim_y + dim_z\n",
        "        self.rnn = nn.GRU(self.embed.embedding_dim, self.dim_h, num_layers=1, dropout=dropout)\n",
        "        # TODO : 두 개의 fc_out 이 필요한 것인가(translation의 경우에) -> 원본 코드에서는 동일한 vocab을 공유하는 듯 하다.\n",
        "        self.fc_out = nn.Linear(self.dim_h, self.embed.num_embeddings) \n",
        "\n",
        "    def forward(self, z, labels, src, src_len, transfered = True):\n",
        "        \"\"\"\n",
        "        Required Parameters\n",
        "            src: original sentence\n",
        "            src_len: original sentence len\n",
        "            TODO : implement beam search?\n",
        "            TODO : unroll up to the length of original sequence length (to be changed if necessary)\n",
        "            TODO : should fc_out() be a module from outside the generator class?(same problem with l.98)\n",
        "            # unroll은 어디까지? end_of_token까지 인가? # 원래 코드는 max_seq 만큼 time step 진행\n",
        "        \n",
        "        * use gumbel_softmax\n",
        "\n",
        "        Returns:\n",
        "            outpus: feed to discriminator\n",
        "            predictions: get loss_rec\n",
        "        \"\"\"\n",
        "        labels = labels.unsqueeze(-1)  # (batch_size, 1)\n",
        "        \n",
        "        # placeholders for outputs and prediction tensors\n",
        "        outputs = torch.zeros(*src.shape, self.dim_h) # outputs = [max_sentence_len, batch_size, dim_h]\n",
        "        predictions = torch.zeros(*src.shape, self.embed.num_embeddings) # g_logits in original code [\",\", vocab size]\n",
        "        \n",
        "        if transfered:\n",
        "            # Feed previous decoding\n",
        "            h0 = torch.cat((self.fc(1-labels), z), -1)  #h0_transfered\n",
        "            \n",
        "            input = self.embed(self.index_sos).repeat(src.shape[1], 1) # <go> or <sos> # batch size = src.shape[1] 만큼 늘리기\n",
        "            input = input.unsqueeze(0)\n",
        "            hidden = h0.unsqueeze(0)                              # [1, batch, hidden_size]\n",
        "            for t in range(1, max(src_len)):                      #TODO: src_len 는 tensor 이기 때문에 그중에 가장 큰것만 사용 \n",
        "                output, hidden = self.rnn(input, hidden)\n",
        "                outputs[t] = output\n",
        "                prediction = self.fc_out(output)    # TODO: 두 개의 다른언어일 경우에 vocab, embeddings 가 각각 2개이고 그 결과 generator도 2개가 있어야 한다. \n",
        "                predictions[t] = prediction\n",
        "                \n",
        "                # 원본코드의 softsample_word를 참조\n",
        "                input = torch.matmul(F.gumbel_softmax(prediction) / self.gamma, self.embed.weight)\n",
        "            \n",
        "\n",
        "        else:\n",
        "            # Teacher Forcing\n",
        "            h0 =  torch.cat((self.fc(labels), z), -1)  #h0_original\n",
        "            input = self.embed(src[0]).unsqueeze(0)    \n",
        "            hidden = h0.unsqueeze(0) # [1, batch_size, hidden_size]\n",
        "            for t in range(1,max(src_len)):    \n",
        "                output, hidden = self.rnn(input, hidden)\n",
        "                outputs[t] = output \n",
        "                prediction = self.fc_out(output)\n",
        "                predictions[t] = prediction # predictions are for calculating loss_rec\n",
        "                input = self.embed(src[t]).unsqueeze(0)\n",
        "        \n",
        "        outputs = torch.cat((h0.unsqueeze(0), outputs), 0) # according to the paper you need h0 in the sequence to feed the discriminator\n",
        "        # outputs = [ sequence_len, batch_size, hidden_size]\n",
        "\n",
        "        return outputs, predictions\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self,dim_h, n_filters, filter_sizes, output_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.cnn = TextCNN(dim_h, n_filters, filter_sizes, output_dim, dropout)\n",
        "        self.criterion_adv = nn.BCELoss()\n",
        "    \n",
        "    def forward(self, h_sequence_real, h_sequence_fake):\n",
        "        d_real = self.cnn(h_sequence_real).squeeze(-1)\n",
        "        d_fake = self.cnn(h_sequence_fake).squeeze(-1)\n",
        "\n",
        "        predictions_real = torch.sigmoid(d_real)\n",
        "        predictions_fake = torch.sigmoid(d_fake)\n",
        "\n",
        "        predictions = torch.cat((predictions_real, predictions_fake), dim = -1)\n",
        "        # predictions = [ batch_size ]\n",
        "\n",
        "        label_real = torch.ones(d_real.size(-1), dtype=torch.float)\n",
        "        label_fake = torch.zeros(d_fake.size(-1), dtype=torch.float)\n",
        "        assert predictions.shape[-1] == label_real.shape[-1] + label_fake.shape[-1]\n",
        "\n",
        "        loss_D = self.criterion_adv(predictions, torch.cat((label_real, label_fake), dim = -1))\n",
        "        loss_G = self.criterion_adv(predictions_fake,label_real)\n",
        "\n",
        "        return loss_D, loss_G\n",
        "\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, dim_h, n_filters, filter_sizes, output_dim, dropout): \n",
        "        # 원본 코드 상의 output_dim은 1\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1,\n",
        "                                              out_channels = n_filters,\n",
        "                                              kernel_size = (fs, dim_h)) \\\n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, hiddens): \n",
        "        # don't forget the permutation\n",
        "        #hiddens = [batch_size, hiddens seq len, dim_h]\n",
        "        hiddens = hiddens.unsqueeze(1)\n",
        "        #hiddens = [batch_size, 1, hiddens seq len, dim_h]\n",
        "\n",
        "        conved = [F.leaky_relu(conv(hiddens)).squeeze(3) for conv in self.convs]\n",
        "        #conved[n] = [batch size, n_filters, dim_h - filter_sizes[n] + 1]\n",
        "\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        #pooled[n] = [batch size, n_filters]\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim =1))\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "\n",
        "        return self.fc(cat)\n",
        "\n",
        "class Transfer(nn.Module):\n",
        "    def __init__(self, pretrained_embeddings, dim_y, dim_z, dropout, \n",
        "                 n_filters, filter_sizes, output_dim, pad_idx=1, sos_idx=2):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
        "\n",
        "        self.encoder = Encoder(self.embed, dim_y, dim_z, dropout)\n",
        "        self.generator = Generator(self.embed, dim_y, dim_z, dropout, temperature, idx_sos=sos_idx)\n",
        "        self.criterion_rec = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "        self.discriminator_0 = Discriminator(dim_y+dim_z, n_filters, filter_sizes, output_dim, dropout)\n",
        "        self.discriminator_1 = Discriminator(dim_y+dim_z, n_filters, filter_sizes, output_dim, dropout)\n",
        "        \n",
        "\n",
        "    def forward(self, text_0, text_0_len, text_1, text_1_len):\n",
        "        labels_0 = torch.zeros(text_0_len.shape[0])\n",
        "        labels_1 = torch.ones(text_1_len.shape[0])\n",
        "\n",
        "        z_0 = self.encoder(labels_0, text_0, text_0_len)\n",
        "        z_1 = self.encoder(labels_1, text_1, text_1_len)\n",
        "\n",
        "        h_ori_seq_0, predictions_ori_0 = self.generator(z_0, labels_0, text_0, text_0_len, transfered=False)\n",
        "        h_trans_seq_1, _  = self.generator(z_1, labels_1, text_0, text_0_len, transfered=True)\n",
        "\n",
        "        h_ori_seq_1, predictions_ori_1 = self.generator(z_1, labels_1, text_1, text_1_len, transfered=False)\n",
        "        h_trans_seq_0, _  = self.generator(z_0, labels_0, text_1, text_1_len, transfered=True)\n",
        "\n",
        "        outputs_0 = predictions_ori_0.view(-1, predictions_ori_0.size(-1))\n",
        "        outputs_1 = predictions_ori_1.view(-1, predictions_ori_1.size(-1))\n",
        "        loss_rec = self.criterion_rec(outputs_0, text_0.view(-1)) + self.criterion_rec(outputs_1, text_1.view(-1))\n",
        "\n",
        "        loss_d0, loss_g0 = self.discriminator_0(h_ori_seq_0, h_trans_seq_1)\n",
        "        loss_d1, loss_g1 = self.discriminator_1(h_ori_seq_1, h_trans_seq_0)\n",
        "        loss_adv = loss_g0 + loss_g1\n",
        "\n",
        "        return loss_rec, loss_adv, loss_d0, loss_d1 \n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "def train(model: Transfer, iterator_0, iterator_1, epochs=20, lr=1e-3, lambda_=1):\n",
        "    assert len(iterator_0) == len(iterator_1), \"the number of batches don't match!\" \n",
        "\n",
        "    temp = \"Epoch: {:3d} | Time: {:.4f} ms | Loss: {:.4f}\"\n",
        "    \n",
        "    optimizer_total = torch.optim.Adam(list(model.encoder.parameters()) + list(model.generator.parameters()),\n",
        "                                       lr = lr)\n",
        "    optimizer_rec = torch.optim.Adam(list(model.encoder.parameters()) + list(model.generator.parameters()),\n",
        "                                          lr = lr)\n",
        "    optimizer_d0 = torch.optim.Adam(model.discriminator_0.parameters(), lr=lr)\n",
        "    optimizer_d1 = torch.optim.Adam(model.discriminator_1.parameters(), lr=lr)\n",
        "    \n",
        "    \n",
        "    list_loss_d0 = []\n",
        "    list_loss_d1 = []\n",
        "    list_loss_total = []\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "\n",
        "        avg_total_loss = 0\n",
        "        for batch_0, batch_1 in tqdm(zip(iterator_0, iterator_1), total=len(iterator_0)):\n",
        "            text_0, text_0_len = batch_0.text0\n",
        "            text_1, text_1_len = batch_1.text1\n",
        "\n",
        "            text_0 = text_0.to(device)\n",
        "            text_1 = text_1.to(device)\n",
        "            text_0_len = text_0_len.to(device)\n",
        "            text_1_len = text_1_len.to(device)\n",
        "\n",
        "            assert text_0_len is not None\n",
        "            assert text_1_len is not None\n",
        "\n",
        "            with torch.autograd.set_detect_anomaly(True):\n",
        "                # Calculating the loss\n",
        "                model.train()\n",
        "                loss_rec, loss_adv, loss_d0, loss_d1 = model(text_0, text_0_len, text_1, text_1_len)\n",
        "                loss_total = loss_rec + lambda_*loss_adv\n",
        "\n",
        "                optimizer_d0.zero_grad()\n",
        "                loss_d0.backward(retain_graph=True)\n",
        "                \n",
        "\n",
        "                optimizer_d1.zero_grad()\n",
        "                loss_d1.backward(retain_graph=True)\n",
        "                \n",
        "                \n",
        "                list_loss_d0.append( loss_d0.item() )\n",
        "                list_loss_d1.append( loss_d1.item() )\n",
        "                #print(\"loss_d0: {:.4f} | loss_d1 {:.4f}\".format(loss_d0.item(), loss_d1.item()))\n",
        "                if loss_d0.item() < 5.5 and loss_d1.item() < 5.5:\n",
        "                    optimizer_total.zero_grad()\n",
        "                    loss_total.backward()\n",
        "                    optimizer_total.step()\n",
        "\n",
        "                    avg_total_loss  += loss_total.item()\n",
        "                    list_loss_total.append( loss_total.item() )\n",
        "                else:\n",
        "                    optimizer_rec.zero_grad()\n",
        "                    loss_rec.backward()\n",
        "                    optimizer_rec.step()\n",
        "\n",
        "                    avg_total_loss += loss_rec.item()\n",
        "                    list_loss_total.append( loss_rec.item() )\n",
        "\n",
        "                optimizer_d0.step()\n",
        "                optimizer_d1.step()\n",
        "                \n",
        "        \n",
        "        avg_total_loss /= len(iterator_0)\n",
        "        elapsed = time.time() - start_time\n",
        "        print(temp.format(epoch + 1, elapsed, avg_total_loss))\n",
        "\n",
        "\n"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkH0qUjQZvpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test and Train the model\n",
        "dim_y = 10\n",
        "dim_z = 30\n",
        "dropout = .1\n",
        "temperature = .0001\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "embed_dim = 100\n",
        "n_filters = 5\n",
        "filter_sizes = [5,4,3,2,1]\n",
        "output_dim=1\n"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAcBUZneyOLS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457,
          "referenced_widgets": [
            "d85b90e409474ab5ba2e9dd620f2823b",
            "18869c3ae36d468585be82a8ae817606",
            "1acfe9561e5d4717930b994fab6848f2",
            "cf7ad306a39a48698f8e7f5b9572f79b",
            "7b249483adc64165ab84fb9aceb4a3d2",
            "0c98eb70c38246e7ac54f566b0a4eef9",
            "a456698fdff54b1aa33e6a26e08cb473",
            "45161e1989954bbeb6ed3efb6f33390b"
          ]
        },
        "outputId": "3e43e3a6-440b-4239-a0c0-03f0bd793194"
      },
      "source": [
        "pad_idx = TEXT.vocab.stoi['<pad>']\n",
        "sos_idx = TEXT.vocab.stoi['<sos>']\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model = Transfer(pretrained_embeddings, dim_y, dim_z, dropout, \n",
        "                     n_filters, filter_sizes, output_dim, \n",
        "                     pad_idx=pad_idx, sos_idx=sos_idx).to(device)\n",
        "    train(model, iterator_0, iterator_1)\n",
        "    "
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d85b90e409474ab5ba2e9dd620f2823b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-168-7bda344e2fab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                      \u001b[0mn_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                      pad_idx=pad_idx, sos_idx=sos_idx).to(device)\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-162-a12d04afc226>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator_0, iterator_1, epochs, lr, lambda_)\u001b[0m\n\u001b[1;32m    280\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mloss_d0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5.5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mloss_d1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                     \u001b[0moptimizer_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m                     \u001b[0mloss_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m                     \u001b[0moptimizer_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLvRlHgj3GbQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Transfer 작성\n",
        "# TODO: Trans\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJlP-ryu50Ar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c9f42bb5-9acb-44e6-85f9-5c6143977ee6"
      },
      "source": [
        "class Person:\n",
        "    def __init__(self, name, age):\n",
        "        self.name = name\n",
        "        self.age = age\n",
        "        self.major = \"blank\"\n",
        "    \n",
        "    def show(self):\n",
        "        print( \"name: \" + self.name + \" age: \"+ str(self.age))\n",
        "    \n",
        "class Student(Person):\n",
        "    def show(self):\n",
        "        print( \"name: \"+ self.name + \" age: \"+ str(self.age) + \" major: \"+ self.major)\n",
        "\n",
        "person = Person(\"david\", 13)\n",
        "student = Student(\"sophie\", 13)\n",
        "\n",
        "person.show()\n",
        "student.show()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name: david age: 13\n",
            "name: sophie age: 13 major: blank\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53XpbvSF77FL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7073070d-a88f-44bd-a220-14453bc4158b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPDwVt_jen1h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b32d8f48-f663-446f-fc71-aa9bfbfdff60"
      },
      "source": [
        "import torch\n",
        "\n",
        "torch.tensor((3,2)).size(-1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dg8gOYmxC3p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2dad79f9-275c-4afd-95ba-889cfbaeef16"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.6.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BABPCkYqTpKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
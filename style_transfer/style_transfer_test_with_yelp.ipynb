{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "id": "HZg60laaY3Ya",
    "outputId": "82a6eeff-7138-4aca-d291-39e42e5481ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 19 21:51:25 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   31C    P0    22W / 250W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  On   | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   32C    P0    24W / 250W |     11MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-PCIE...  On   | 00000000:00:08.0 Off |                    0 |\n",
      "| N/A   70C    P0   217W / 250W |  25630MiB / 32480MiB |     92%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    2     26735      C   python                                     25615MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5nhfag33ZSep",
    "outputId": "cb046c6b-2b31-4ccb-85fd-ad0481a217c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215,
     "referenced_widgets": [
      "5a388f3579c14240a8fe0baa4d1d3470",
      "b17fa1bd97ae4da0b18cd9d1034f7603",
      "7ee59baff9c5498f9cf7088f5edfab87",
      "603037b89b8b4714953c9b772b14117c",
      "f695337c911c43f19c1cf4ed1a561db2",
      "e2b812e0fccc49c881912b6ca5eca355",
      "2807e02861f44f8d81e8cbdbcc06605a",
      "3587b5eaad2d4a76a531b4cd80979dbe",
      "ca6ca7cdbb9b4a7ca6ede097e1f8d520",
      "79082d4a49894187991aeeeb7b23f3d3",
      "8182ccacb5e24aaa907b3ba2b931c765",
      "3fa60069055c44eda55004eb0a2265db",
      "41456b7c47594d54809e813f66585ff5",
      "2368234822a3434e8be9b380f47c3b50",
      "0471f46f967f428d935aec2c1c373e5b",
      "7564f2f1845e47e7a1d531804209755d"
     ]
    },
    "colab_type": "code",
    "id": "PW_6MKQgeo5C",
    "outputId": "4547a04a-2de5-4215-84a4-035d0f66bd38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smaller: 38205\n",
      "larger: 38205\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c969e17e7f74571a79fcd50eef34ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e38ed41c664e118467c1534b03b32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "data length: 38205\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "path_0 = \"sentiment.dev.0\"\n",
    "path_1 = \"sentiment.dev.1\"\n",
    "\n",
    "def equalize_seq_num(path_0, path_1):\n",
    "    \"\"\"\n",
    "    TorchText Bucket Iterator 내부에서 batch 개수를 똑같이 맞출 수가 없어서,\n",
    "    일단 pandas를 이용해서 sequence의 개수를 동일하게 먼저 만들어 준 후에,\n",
    "    파일을 저장해서 사용한다. 태완님이 좋은 코드를 짜주셨기를 바라면서...\n",
    "    \"\"\"\n",
    "    data0 = pd.read_table(path_0, header=None)\n",
    "    data1 = pd.read_table(path_1, header=None)\n",
    "\n",
    "    if len(data0) > len(data1):\n",
    "        larger, smaller = data0, data1\n",
    "    else:\n",
    "        larger, smaller = data1, data0\n",
    "\n",
    "    print(\"smaller: \" + str(len(smaller)))\n",
    "    print(\"larger: \" + str(len(larger)))\n",
    "\n",
    "    repeat_num = int( (len(larger) - len(smaller)) / len(smaller))\n",
    "    print(repeat_num)\n",
    "    remain_num = (len(larger)-len(smaller)) % len(smaller)\n",
    "    print(remain_num)\n",
    "    \n",
    "\n",
    "    for i in tqdm(range(repeat_num)):\n",
    "        smaller = smaller.append(smaller, ignore_index=True)\n",
    "    \n",
    "    len_small = len(smaller)\n",
    "    for i in tqdm(range(remain_num)):\n",
    "        smaller = smaller.append(smaller.loc[i%len_small], ignore_index=True)\n",
    "\n",
    "\n",
    "    #for i in tqdm(range(len(larger) - len(smaller))):\n",
    "    #    smaller = smaller.append(smaller.loc[i%len_small], ignore_index=True)\n",
    "    #    i+=1\n",
    "    print(\"data length: {}\".format(len(smaller)))\n",
    "    assert len(larger)==len(smaller)\n",
    "\n",
    "    if len(data0) > len(data1):\n",
    "        larger.to_csv(path_0, header=None, index=None, sep=' ')\n",
    "        smaller.to_csv(path_1, header=None, index=None, sep=' ')\n",
    "    else:\n",
    "        larger.to_csv(path_1, header=None, index=None, sep=' ')\n",
    "        smaller.to_csv(path_0, header=None, index=None, sep=' ')       \n",
    "    \n",
    "    \n",
    "equalize_seq_num(path_0, path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Prg9b248oASF",
    "outputId": "0b732398-f922-408d-8efd-5a3779ea767d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38205\t38205\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "data0 = pd.read_table(path_0, header=None)\n",
    "data1 = pd.read_table(path_1, header=None)\n",
    "\n",
    "print(len(data0), len(data1), sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cu_f8U5Pws2G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:31, 2.20MB/s]                               \n",
      "100%|█████████▉| 399418/400000 [00:19<00:00, 19546.92it/s]"
     ]
    }
   ],
   "source": [
    "import torchtext.data as data\n",
    "\n",
    "class BucketIterator_complete_last(data.BucketIterator): # return last batch of batch_size\n",
    "    \"\"\"\n",
    "        when last batch is not batch_sized Error occurs in rnn models.\n",
    "        need to either drop or fill in the last batch.\n",
    "        this is a modified code from BucketIterator, overloading \"BucketIterator.batch()\" method.\n",
    "    \"\"\"\n",
    "    def batch(data, batch_size, batch_size_fn=None):\n",
    "        \"\"\"Yield elements from data in chunks of batch_size.\"\"\"\n",
    "        if batch_size_fn is None:\n",
    "            def batch_size_fn(new, count, sofar):\n",
    "                return count\n",
    "        minibatch, size_so_far = [], 0\n",
    "        for ex in data:\n",
    "            minibatch.append(ex)\n",
    "            size_so_far = batch_size_fn(ex, len(minibatch), size_so_far)\n",
    "            if size_so_far == batch_size:\n",
    "                yield minibatch\n",
    "                minibatch, size_so_far = [], 0\n",
    "            elif size_so_far > batch_size:\n",
    "                yield minibatch[:-1]\n",
    "                minibatch, size_so_far = minibatch[-1:], batch_size_fn(ex, 1, 0)\n",
    "\n",
    "        if minibatch and size_so_far < batch_size:\n",
    "            for ex in data[:batch_size - size_so_far]:\n",
    "                minibatch.append(ex)\n",
    "            yield minibatch\n",
    "        if minibatch:\n",
    "            yield minibatch\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    if text[-1]==\".\":\n",
    "        text.pop(-1)\n",
    "    return text\n",
    "\n",
    "# Init Field\n",
    "\n",
    "TEXT = data.Field(\n",
    "    tokenize=\"spacy\",\n",
    "    preprocessing=preprocessing,\n",
    "    init_token = '<sos>',\n",
    "    eos_token = '<eos>',\n",
    "    include_lengths=True,\n",
    "    lower = True\n",
    ")\n",
    "\n",
    "# Init TabularDataset\n",
    "dataset0 = data.TabularDataset(\n",
    "    path='sentiment.dev.0',\n",
    "    format='tsv',\n",
    "    fields=[('text0',TEXT)],\n",
    ")\n",
    "dataset1 = data.TabularDataset(\n",
    "    path='sentiment.dev.1',\n",
    "    format='tsv',\n",
    "    fields=[('text1',TEXT)]\n",
    ")\n",
    "\n",
    "## Build Vocab\n",
    "\n",
    "TEXT.build_vocab(dataset0, dataset1, min_freq=3, vectors=\"glove.6B.100d\")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "## make iterator\n",
    "iterator_0=BucketIterator_complete_last(\n",
    "    dataset0, \n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x : len(x.text0),\n",
    ")\n",
    "\n",
    "iterator_1=BucketIterator_complete_last(\n",
    "    dataset1, \n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x : len(x.text1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "ux49VET-NvV7",
    "outputId": "e2fabc78-4298-4ba6-afa5-1235a6a6c0f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ok', 'never', 'going', 'back', 'to', 'this', 'place', 'again']\n",
      "['easter', 'day', 'nothing', 'open', ',', 'heard', 'about', 'this', 'place', 'figured', 'it', 'would', 'ok']\n",
      "['the', 'host', 'that', 'walked', 'us', 'to', 'the', 'table', 'and', 'left', 'without', 'a', 'word']\n",
      "['it', 'just', 'gets', 'worse']\n",
      "['the', 'food', 'tasted', 'awful']\n",
      "['no', 'sign', 'of', 'the', 'manager']\n",
      "['the', 'last', 'couple', 'years', 'this', 'place', 'has', 'been', 'going', 'down', 'hill']\n",
      "['last', 'night', 'however', 'it', 'was', 'way', 'to', 'thick', 'and', 'tasteless']\n",
      "['it', 'smelled', 'like', 'rotten', 'urine']\n",
      "['i', 'am', 'not', 'exaggerating']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████▉| 399418/400000 [00:30<00:00, 19546.92it/s]"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(dataset0[i].text0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I9O5ck_a1aRJ"
   },
   "outputs": [],
   "source": [
    "# Models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding, dim_y, dim_z, dropout):\n",
    "        \"\"\"\n",
    "        Required parameters:\n",
    "            embedding: nn.Embedding\n",
    "            dim_y: hyperparam\n",
    "            dim_z: hyperparam\n",
    "        \n",
    "        구성요소:\n",
    "            Embedding Layer\n",
    "            Fully connected Layer (to get latent variable `y`)\n",
    "            unidirectional GRU with layer 1\n",
    "            \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear( 1, dim_y).to(device) # output: (batch_size, dim_y)\n",
    "        self.init_z = torch.zeros(dim_z).to(device)\n",
    "        self.embed = embedding\n",
    "\n",
    "        self.rnn = nn.GRU(self.embed.embedding_dim, dim_y + dim_z, num_layers=1, dropout=dropout)\n",
    "        self.dim_y = dim_y\n",
    "    \n",
    "    def forward(self, labels, src, src_len ):\n",
    "        labels = labels.unsqueeze(-1).to(device) # (batch_size, 1), 엔코더 밖에서 해줘도 괜찮을 듯\n",
    "        src = self.embed(src)\n",
    "        packed_embed = nn.utils.rnn.pack_padded_sequence(src, src_len) # input input to rnn\n",
    "        \n",
    "        # initial hidden state of the encoder: concat ( y, z)   \n",
    "        \n",
    "        init_z = self.init_z.repeat(src.shape[1], 1)#.to(device) # [ batch size: src.shape[1] , dim_z ]\n",
    "        init_hidden = torch.cat((self.fc(labels), init_z), -1) \n",
    "\n",
    "        _, hidden = self.rnn(packed_embed, init_hidden.unsqueeze(0))\n",
    "        # hidden : hidden_state of the final time step\n",
    "        hidden = hidden.squeeze(0)\n",
    "        z = hidden[:, self.dim_y:]\n",
    "        return z\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, embeddings, dim_y, dim_z, dropout, temperature, idx_sos=2):\n",
    "         \"\"\"\n",
    "        Required parameters:\n",
    "            embedding: nn.Embedding()\n",
    "            dim_y: .\n",
    "            dim_z: .\n",
    "            dropout: refer to paper\n",
    "            temperature: refer to paper\n",
    "            idx_sos: TEXT.vocab.stoi['<sos>']\n",
    "        \n",
    "        Components:\n",
    "            Fully connected Layer (to get latent `y`)\n",
    "            Word Embedding \n",
    "            Unidirectional GRU (layer=1)\n",
    "            Fully connected Layer (prediction)\n",
    "        \"\"\"\n",
    "        super().__init__() \n",
    "        self.gamma = temperature\n",
    "        self.dim_h = dim_y + dim_z\n",
    "\n",
    "        self.embed = embeddings # type(embeddings) = nn.Embedding\n",
    "        self.index_sos = torch.tensor([idx_sos],dtype=int).to(device) # to feed <sos> when generating a transfered text\n",
    "\n",
    "        self.fc = nn.Linear(1, dim_y) # latent `y`\n",
    "        # The hidden state's dimension: dim_y + dim_z\n",
    "        self.rnn = nn.GRU(self.embed.embedding_dim, self.dim_h, num_layers=1, dropout=dropout)\n",
    "        # TODO : 두 개의 fc_out 이 필요한 것인가(translation의 경우에)\n",
    "        self.fc_out = nn.Linear(self.dim_h, self.embed.num_embeddings) # prediction\n",
    "\n",
    "    def forward(self, z, labels, src, src_len, transfered = True):\n",
    "        \"\"\"\n",
    "        Required Parameters\n",
    "            src: original sentence [seq_len, batch_size]\n",
    "            src_len: original sentence len [batch_size]\n",
    "            \n",
    "            TODO : implement beam search?\n",
    "            TODO : unroll up to the length of original sequence length (to be changed if necessary)\n",
    "            TODO : should fc_out() be a module from outside the generator class?(same problem with l.98)\n",
    "            # unroll은 어디까지? end_of_token까지 인가? # 원래 코드는 max_seq 만큼 time step 진행\n",
    "        \n",
    "        * using gumbel_softmax\n",
    "\n",
    "        Returns:\n",
    "            outpus: to feed to discriminator\n",
    "            predictions: get loss_rec\n",
    "        \"\"\"\n",
    "        labels = labels.unsqueeze(-1).to(device)  # (batch_size, 1)\n",
    "        \n",
    "        # placeholders for outputs and prediction tensors\n",
    "        outputs = torch.zeros(*src.shape, self.dim_h).to(device) # outputs = [max_sentence_len, batch_size, dim_h]\n",
    "        predictions = torch.zeros(*src.shape, self.embed.num_embeddings).to(device) # g_logits in original code [\",\", vocab size]\n",
    "        \n",
    "        if transfered:\n",
    "            # Feed previous decoding\n",
    "            h0 = torch.cat((self.fc(1-labels), z), -1)  #h0_transfered\n",
    "            \n",
    "            input = self.embed(self.index_sos).repeat(src.shape[1], 1) # <go> or <sos> # batch size = src.shape[1] 만큼 늘리기\n",
    "            input = input.unsqueeze(0)\n",
    "            hidden = h0.unsqueeze(0)                              # [1, batch, hidden_size]\n",
    "            for t in range(1, max(src_len)):                      #TODO: src_len 는 tensor 이기 때문에 그중에 가장 큰것만 사용 \n",
    "                output, hidden = self.rnn(input, hidden)\n",
    "                outputs[t] = output\n",
    "                prediction = self.fc_out(output)    # TODO: 두 개의 다른언어일 경우에 vocab, embeddings 가 각각 2개이고 그 결과 generator도 2개가 있어야 한다. \n",
    "                predictions[t] = prediction\n",
    "                \n",
    "                # 원본코드의 softsample_word를 참조\n",
    "                input = torch.matmul(F.gumbel_softmax(prediction) / self.gamma, self.embed.weight)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # Teacher Forcing\n",
    "            h0 =  torch.cat((self.fc(labels), z), -1)  #h0_original\n",
    "            input = self.embed(src[0]).unsqueeze(0)    \n",
    "            hidden = h0.unsqueeze(0) # [1, batch_size, hidden_size]\n",
    "            for t in range(1,max(src_len)):    \n",
    "                output, hidden = self.rnn(input, hidden)\n",
    "                outputs[t] = output \n",
    "                prediction = self.fc_out(output)\n",
    "                predictions[t] = prediction # predictions are for calculating loss_rec\n",
    "                input = self.embed(src[t]).unsqueeze(0)\n",
    "        \n",
    "        outputs = torch.cat((h0.unsqueeze(0), outputs), 0) # according to the paper you need h0 in the sequence to feed the discriminator\n",
    "        # outputs = [ sequence_len, batch_size, hidden_size]\n",
    "\n",
    "        return outputs, predictions\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,dim_h, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.cnn = TextCNN(dim_h, n_filters, filter_sizes, output_dim, dropout)\n",
    "        self.criterion_adv = nn.BCELoss()\n",
    "    \n",
    "    def forward(self, h_sequence_real, h_sequence_fake):\n",
    "        d_real = self.cnn(h_sequence_real).squeeze(-1)\n",
    "        d_fake = self.cnn(h_sequence_fake).squeeze(-1)\n",
    "        \n",
    "        # 임의로 지정해줘도 된다. 어차피 Binary_Problem\n",
    "        predictions_real = torch.sigmoid(d_real)\n",
    "        predictions_fake = torch.sigmoid(d_fake)\n",
    "\n",
    "        predictions = torch.cat((predictions_real, predictions_fake), dim = -1)\n",
    "        # predictions = [ batch_size ]\n",
    "\n",
    "        label_real = torch.ones(d_real.size(-1), dtype=torch.float).to(device)\n",
    "        label_fake = torch.zeros(d_fake.size(-1), dtype=torch.float).to(device)\n",
    "        assert predictions.shape[-1] == label_real.shape[-1] + label_fake.shape[-1]\n",
    "        \n",
    "        # loss_D is for optimizing params from Discriminator\n",
    "        # loss_G is for optimizing params from Encoder & Generator\n",
    "        loss_D = self.criterion_adv(predictions, torch.cat((label_real, label_fake), dim = -1))\n",
    "        loss_G = self.criterion_adv(predictions_fake,label_real)\n",
    "\n",
    "        return loss_D, loss_G\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, dim_h, n_filters, filter_sizes, output_dim, dropout): \n",
    "        # 원본 코드 상의 output_dim은 1\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1,\n",
    "                                              out_channels = n_filters,\n",
    "                                              kernel_size = (fs, dim_h)) \\\n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, hiddens): \n",
    "        # don't forget the permutation\n",
    "        #hiddens = [batch_size, hiddens seq len, dim_h]\n",
    "        hiddens = hiddens.unsqueeze(1)\n",
    "        #hiddens = [batch_size, 1, hiddens seq len, dim_h]\n",
    "\n",
    "        conved = [F.leaky_relu(conv(hiddens)).squeeze(3) for conv in self.convs]\n",
    "        #conved[n] = [batch size, n_filters, dim_h - filter_sizes[n] + 1]\n",
    "\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        #pooled[n] = [batch size, n_filters]\n",
    "\n",
    "        cat = self.dropout(torch.cat(pooled, dim =1))\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "\n",
    "        return self.fc(cat)\n",
    "\n",
    "class Transfer(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, dim_y, dim_z, dropout, \n",
    "                 n_filters, filter_sizes, output_dim, pad_idx=1, sos_idx=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        \n",
    "        # Encoder, Generator, Discriminator_0, Discriminator_1, loss_rec\n",
    "        # loss_d0, loss_d1, loss_adv(loss_g0+loss_g1) is constructed in Dicriminator\n",
    "        self.encoder = Encoder(self.embed, dim_y, dim_z, dropout)\n",
    "        self.generator = Generator(self.embed, dim_y, dim_z, dropout, temperature, idx_sos=sos_idx)\n",
    "\n",
    "        self.discriminator_0 = Discriminator(dim_y+dim_z, n_filters, filter_sizes, output_dim, dropout)\n",
    "        self.discriminator_1 = Discriminator(dim_y+dim_z, n_filters, filter_sizes, output_dim, dropout)\n",
    "\n",
    "        self.criterion_rec = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "    def forward(self, text_0, text_0_len, text_1, text_1_len):\n",
    "        # set labels for texts from two different styles(domain)\n",
    "        labels_0 = torch.zeros(text_0_len.shape[0])\n",
    "        labels_1 = torch.ones(text_1_len.shape[0])\n",
    "        \n",
    "        # Get z0,z1, h0s, h1s -> compute loss_rec, loss_adv, loss_d0, loss_d1\n",
    "        z_0 = self.encoder(labels_0, text_0, text_0_len)\n",
    "        z_1 = self.encoder(labels_1, text_1, text_1_len)\n",
    "\n",
    "        h_ori_seq_0, predictions_ori_0 = self.generator(z_0, labels_0, text_0, text_0_len, transfered=False)\n",
    "        h_trans_seq_1, _  = self.generator(z_1, labels_1, text_0, text_0_len, transfered=True)\n",
    "\n",
    "        h_ori_seq_1, predictions_ori_1 = self.generator(z_1, labels_1, text_1, text_1_len, transfered=False)\n",
    "        h_trans_seq_0, _  = self.generator(z_0, labels_0, text_1, text_1_len, transfered=True)\n",
    "        \n",
    "        outputs_0 = predictions_ori_0.view(-1, predictions_ori_0.size(-1))\n",
    "        outputs_1 = predictions_ori_1.view(-1, predictions_ori_1.size(-1))\n",
    "        loss_rec = self.criterion_rec(outputs_0, text_0.view(-1)) + self.criterion_rec(outputs_1, text_1.view(-1))\n",
    "        \n",
    "        loss_d0, loss_g0 = self.discriminator_0(h_ori_seq_0, h_trans_seq_1)\n",
    "        loss_d1, loss_g1 = self.discriminator_1(h_ori_seq_1, h_trans_seq_0)\n",
    "        loss_adv = loss_g0 + loss_g1\n",
    "\n",
    "        return loss_rec, loss_adv, loss_d0, loss_d1 \n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "def train(model: Transfer, iterator_0, iterator_1, epochs=20, lr=1e-3, lambda_=1):\n",
    "    assert len(iterator_0) == len(iterator_1), \"the number of batches don't match!\" \n",
    "\n",
    "    temp = \"Epoch: {:3d} | Time: {:.4f} ms | Loss: {:.4f}\"\n",
    "    \n",
    "    optimizer_total = torch.optim.Adam(list(model.encoder.parameters()) + list(model.generator.parameters()),\n",
    "                                       lr = lr)\n",
    "    optimizer_rec = torch.optim.Adam(list(model.encoder.parameters()) + list(model.generator.parameters()),\n",
    "                                          lr = lr)\n",
    "    optimizer_d0 = torch.optim.Adam(model.discriminator_0.parameters(), lr=lr)\n",
    "    optimizer_d1 = torch.optim.Adam(model.discriminator_1.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    list_loss_d0 = []\n",
    "    list_loss_d1 = []\n",
    "    list_loss_total = []\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        avg_total_loss = 0\n",
    "        for batch_0, batch_1 in tqdm(zip(iterator_0, iterator_1), total=len(iterator_0)):\n",
    "            text_0, text_0_len = batch_0.text0\n",
    "            text_1, text_1_len = batch_1.text1\n",
    "\n",
    "            text_0 = text_0.to(device)\n",
    "            text_1 = text_1.to(device)\n",
    "            text_0_len = text_0_len.to(device)\n",
    "            text_1_len = text_1_len.to(device)\n",
    "\n",
    "            assert text_0_len is not None\n",
    "            assert text_1_len is not None\n",
    "\n",
    "            with torch.autograd.set_detect_anomaly(True):\n",
    "                # Calculating the loss\n",
    "                model.train()\n",
    "                loss_rec, loss_adv, loss_d0, loss_d1 = model(text_0, text_0_len, text_1, text_1_len)\n",
    "                loss_total = loss_rec + lambda_*loss_adv\n",
    "\n",
    "                optimizer_d0.zero_grad()\n",
    "                loss_d0.backward(retain_graph=True)\n",
    "                \n",
    "\n",
    "                optimizer_d1.zero_grad()\n",
    "                loss_d1.backward(retain_graph=True)\n",
    "                \n",
    "                \n",
    "                list_loss_d0.append( loss_d0.item() )\n",
    "                list_loss_d1.append( loss_d1.item() )\n",
    "                #print(\"loss_d0: {:.4f} | loss_d1 {:.4f}\".format(loss_d0.item(), loss_d1.item()))\n",
    "                if loss_d0.item() < 5.5 and loss_d1.item() < 5.5:\n",
    "                    optimizer_total.zero_grad()\n",
    "                    loss_total.backward()\n",
    "                    optimizer_total.step()\n",
    "\n",
    "                    avg_total_loss  += loss_total.item()\n",
    "                    list_loss_total.append( loss_total.item() )\n",
    "                else:\n",
    "                    optimizer_rec.zero_grad()\n",
    "                    loss_rec.backward()\n",
    "                    optimizer_rec.step()\n",
    "\n",
    "                    avg_total_loss += loss_rec.item()\n",
    "                    list_loss_total.append( loss_rec.item() )\n",
    "\n",
    "                optimizer_d0.step()\n",
    "                optimizer_d1.step()\n",
    "                \n",
    "        \n",
    "        avg_total_loss /= len(iterator_0)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(temp.format(epoch + 1, elapsed, avg_total_loss))\n",
    "\n",
    "# 최종적으로 변경한 사항\n",
    "# label.to(device) 부분을 Transfer 에서 Encdoer, Decoder 부분으로 옮겼음: for better modularity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XkH0qUjQZvpH"
   },
   "outputs": [],
   "source": [
    "# Test and Train the model\n",
    "dim_y = 10\n",
    "dim_z = 30\n",
    "dropout = .1\n",
    "temperature = .0001\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "embed_dim = 100\n",
    "n_filters = 5\n",
    "filter_sizes = [5,4,3,2,1]\n",
    "output_dim=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457,
     "referenced_widgets": [
      "d85b90e409474ab5ba2e9dd620f2823b",
      "18869c3ae36d468585be82a8ae817606",
      "1acfe9561e5d4717930b994fab6848f2",
      "cf7ad306a39a48698f8e7f5b9572f79b",
      "7b249483adc64165ab84fb9aceb4a3d2",
      "0c98eb70c38246e7ac54f566b0a4eef9",
      "a456698fdff54b1aa33e6a26e08cb473",
      "45161e1989954bbeb6ed3efb6f33390b"
     ]
    },
    "colab_type": "code",
    "id": "rAcBUZneyOLS",
    "outputId": "3e43e3a6-440b-4239-a0c0-03f0bd793194"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3732bc8fdea142a7b81b1b956ce2f6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   1 | Time: 659.6019 ms | Loss: 13.8743\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f6472e43ca418bb570d3e5116e761d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   2 | Time: 655.9768 ms | Loss: 12.5171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b7a027fa254415a2dcad8d67641731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   3 | Time: 654.9843 ms | Loss: 12.1118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d24520c94374f4a8e64f5f9e17eeaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   4 | Time: 653.6894 ms | Loss: 11.8007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a9779d865043eeaec256282cc9bbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   5 | Time: 659.9756 ms | Loss: 11.5233\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6f52edf15b4d079f7e060d5ec65470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   6 | Time: 657.8969 ms | Loss: 11.3170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad89fde184944da0a8e090a13bcbdd2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   7 | Time: 654.2396 ms | Loss: 10.9929\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55880c30ce4644fd90def9e4ea6a8c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   8 | Time: 658.5396 ms | Loss: 10.6797\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013c2dca7bc24e21889ed3d57f7913d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   9 | Time: 649.1224 ms | Loss: 10.4060\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae170db0df234d8ab2446f31e8d70b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  10 | Time: 652.0539 ms | Loss: 10.2035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fd12e168dfa4c488ac6ca99fa4b4360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  11 | Time: 653.9613 ms | Loss: 10.0263\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9db8e215ffe947cdac6234b9abc25f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  12 | Time: 655.9452 ms | Loss: 9.8397\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5853a7d17c441ce81888995d52f0cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  13 | Time: 653.3778 ms | Loss: 9.7808\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11050d3596f6472bb35b7659ed5b91c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  14 | Time: 652.1547 ms | Loss: 9.7576\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c477e85d22544f449d00f263b5f83ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  15 | Time: 657.2159 ms | Loss: 9.5826\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea4c6f3944f49ed9090624c1fa1cdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  16 | Time: 653.3493 ms | Loss: 9.5002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085f2e1df02c4f38a7e002d56d9b6c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  17 | Time: 658.0529 ms | Loss: 9.5153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632d901b337d4c33a499ffdb55873e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  18 | Time: 658.8557 ms | Loss: 9.3959\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e209cfd3684160934188e9b476ea81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  19 | Time: 649.4509 ms | Loss: 9.3287\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa6a8d294fd417082a0b465a2f08009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1194.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  20 | Time: 649.1791 ms | Loss: 9.2979\n"
     ]
    }
   ],
   "source": [
    "pad_idx = TEXT.vocab.stoi['<pad>']\n",
    "sos_idx = TEXT.vocab.stoi['<sos>']\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = Transfer(pretrained_embeddings, dim_y, dim_z, dropout, \n",
    "                     n_filters, filter_sizes, output_dim, \n",
    "                     pad_idx=pad_idx, sos_idx=sos_idx).to(device)\n",
    "    train(model, iterator_0, iterator_1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "53XpbvSF77FL",
    "outputId": "7073070d-a88f-44bd-a220-14453bc4158b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_data\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JPDwVt_jen1h",
    "outputId": "b32d8f48-f663-446f-fc71-aa9bfbfdff60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.tensor((3,2)).size(-1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "style-transfer-test-with-yolo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Environment (torchtext)",
   "language": "python",
   "name": "ex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0471f46f967f428d935aec2c1c373e5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c98eb70c38246e7ac54f566b0a4eef9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18869c3ae36d468585be82a8ae817606": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1acfe9561e5d4717930b994fab6848f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "  3%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c98eb70c38246e7ac54f566b0a4eef9",
      "max": 1194,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7b249483adc64165ab84fb9aceb4a3d2",
      "value": 35
     }
    },
    "2368234822a3434e8be9b380f47c3b50": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2807e02861f44f8d81e8cbdbcc06605a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3587b5eaad2d4a76a531b4cd80979dbe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3fa60069055c44eda55004eb0a2265db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7564f2f1845e47e7a1d531804209755d",
      "placeholder": "​",
      "style": "IPY_MODEL_0471f46f967f428d935aec2c1c373e5b",
      "value": " 12927/12927 [00:43&lt;00:00, 295.98it/s]"
     }
    },
    "41456b7c47594d54809e813f66585ff5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "45161e1989954bbeb6ed3efb6f33390b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a388f3579c14240a8fe0baa4d1d3470": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7ee59baff9c5498f9cf7088f5edfab87",
       "IPY_MODEL_603037b89b8b4714953c9b772b14117c"
      ],
      "layout": "IPY_MODEL_b17fa1bd97ae4da0b18cd9d1034f7603"
     }
    },
    "603037b89b8b4714953c9b772b14117c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3587b5eaad2d4a76a531b4cd80979dbe",
      "placeholder": "​",
      "style": "IPY_MODEL_2807e02861f44f8d81e8cbdbcc06605a",
      "value": " 0/0 [00:47&lt;?, ?it/s]"
     }
    },
    "7564f2f1845e47e7a1d531804209755d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79082d4a49894187991aeeeb7b23f3d3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b249483adc64165ab84fb9aceb4a3d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7ee59baff9c5498f9cf7088f5edfab87": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2b812e0fccc49c881912b6ca5eca355",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f695337c911c43f19c1cf4ed1a561db2",
      "value": 0
     }
    },
    "8182ccacb5e24aaa907b3ba2b931c765": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2368234822a3434e8be9b380f47c3b50",
      "max": 12927,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_41456b7c47594d54809e813f66585ff5",
      "value": 12927
     }
    },
    "a456698fdff54b1aa33e6a26e08cb473": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b17fa1bd97ae4da0b18cd9d1034f7603": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca6ca7cdbb9b4a7ca6ede097e1f8d520": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8182ccacb5e24aaa907b3ba2b931c765",
       "IPY_MODEL_3fa60069055c44eda55004eb0a2265db"
      ],
      "layout": "IPY_MODEL_79082d4a49894187991aeeeb7b23f3d3"
     }
    },
    "cf7ad306a39a48698f8e7f5b9572f79b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45161e1989954bbeb6ed3efb6f33390b",
      "placeholder": "​",
      "style": "IPY_MODEL_a456698fdff54b1aa33e6a26e08cb473",
      "value": " 35/1194 [01:06&lt;30:36,  1.58s/it]"
     }
    },
    "d85b90e409474ab5ba2e9dd620f2823b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1acfe9561e5d4717930b994fab6848f2",
       "IPY_MODEL_cf7ad306a39a48698f8e7f5b9572f79b"
      ],
      "layout": "IPY_MODEL_18869c3ae36d468585be82a8ae817606"
     }
    },
    "e2b812e0fccc49c881912b6ca5eca355": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f695337c911c43f19c1cf4ed1a561db2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

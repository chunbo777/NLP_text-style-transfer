# -*- coding: utf-8 -*-
"""language_style_transfer_pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FUOScfQsLlgRTBD-FyZzMv9kkwUw4W0C

## Modules

- Encoder
- Generator(RNN)
- Discriminator
"""

# 문제점 나열
# RNN size는 어떻게? tf.dynamic_rnn은 대체 무엇인가 -> multiple sized sentences 를 처리하기 위함
# 테스트를 위해 글로브 엠베딩을 사용한다.

import torch
import torch.nn as nn
import torch.nn.functional as F

class Encoder(nn.Module):
    def __init__(batch_size, dim_y, dim_z):
        """
        Required parameters:
            batch_size, dim_z, dim_y, embed_dim, labels
        """
        super().__init__()
        self.fc = nn.Linear( 1, dim_y) # output: (batch_size, dim_y)
        self.init_z = torch.zeros(batch_size, dim_z) #QUESTION: initial state은 안바뀌는게 맞는가?
        
        self.rnn = nn.GRU(embed_dim, dim_y + dim_z,num_layers=1)
        # TODO : dropout 추가 원본 코드에 있음
    
    def forward(self, src, src_len, labels):
        labels = labels_in.unsqueeze(-1) # (batch_size, 1), 엔코더 밖에서 해줘도 괜찮을 듯
        packed_embed = nn.utils.rnn.pack_padded_sequence(embed, src_len) # input input to rnn
        
        # initial hidden state of the encoder :cat ( y, z)
        # QUESTION: y를 만드는 linear fuction 의 파라미터도 학습의 대상이 되는게 맞겠지?
        init_hidden = torch.cat(self.fc(labels), self.init_z) 
        packed_outputs, hidden = self.rnn(packed_embed, init_hidden)
        
        #outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)
        z = hidden[:, dim_y:]
        return z

class Generator(nn.Module):
    def __init__(self,batch_size, dim_y, dim_z, temperature):
        """
        Required Parameters:
            all the same exept "z" which is the output from the Encoder
        """
        super().__init__() 
        self.gamma = temperature
        self.dim_h = dim_y + dim_z

        self.fc = nn.Linear(1, dim_y)
        # The hidden state's dimension: dim_y + dim_z
        self.rnn = nn.GRU(embed_dim, self.dim_h, num_layers=1)
        self.fc_out = nn.Linear(self.dim_h, vocab_dim) # vocab_dim = TRG.vocab.shape[0]

    def forward(self, labels, z, src, src_len, transfered = True)
        """
        Required Parameters
            z : output from the encoder
            src, src_len : for teacher forcing the original sentence

        z, labels, 를 이용해서 inital hidden을 Genrator 앞에다가 넣어줘야 한다.
        """
        #QUESTION: Packed padded sequence를 decoder부분에서도 써줘야 하는가?
        #QUESTION: Max_len 어떻게 처리할래?

        src = self.embed(src)

        #QUESTION: Decoder의 input은 하나하나 씩 들어가는데, batch 중에서 빨리 끝나는 문장은 어떻게 되는 것인가?
        # 그냥 그대로 0 padding input으로 계속 받으면서 기다리는 건가?
        # 일단은 trans, original 모두 src_len 만큼 unroll 되도록했다.
        
        # unroll은 어디까지? end_of_token까지 인가?
        hiddens = torch.zeros(*src.shape, self.dim_h) # 원래 코드는 max_seq 만큼 time step 진행
        outputs = torch.zeros(*src.shape, self.dim_h)
        predictions = torch.zeros(*src.shape, len(TRG.vocab)) # g_logits in original code
        
        if transfered:
            # using softmax to feed previous decoding
            h0_transfered = torch.cat(self.fc(1-labels), z) #h0_transfered
            
            input = src[0]
            hidden = h0_transfered
            for t in range(1, src_len):
                output, hidden, prediction = self.rnn(input, hidden)
                outputs[t] = output # 여기 사실 outputs[:,t] 로 했어야 하는 것 아닌가? seq2seq 그대로 쓰긴했는데... 나중에 테스트 할 때 확인
                prediction = self.fc_out(output)
                predictions[t] = prediction
                input = nn.Softmax(prediction) / self.gamma

        else:
            h0_original =  torch.cat(self.fc(labels), z) #h0_original
            # using teacher forcing
            input = TRG.vocab['<go>']# <go> or <sos>
            hidden = h0_original
            for t in range(1,src_len):
                output, hidden = self.rnn(input, hidden)
                outputs[t] = output 
                prediction = self.fc_out(output)
                predictions[t] = prediction
                input = src[t]
            
        return outputs, hiddens, predictions # predictions는 loss_rec 를구하기 위함

class Discriminator(nn.Module):
    def __init__(self,dim_h, n_filters, filter_sizes, output_dim, dropout):
        super().__init__()
        self.cnn = TextCNN(dim_h, n_filters, filter_sizes, output_dim, dropout)
        self.criterion_adv = nn.BCELoss()
    
    def forward(self, x_real, x_fake):
        d_real = self.cnn(x_real)
        d_fake = self.cnn(x_fake)
        predictions_real = F.sigmoid(d_real) 
        predictions_fake = F.sigmoid(d_fake)
        predictions = torch.cat((predictions_real, predictions_fake), dim = -1)
        # predictions = [ batch_size ]

        label_real = torch.ones(len(predictions_real), dtype=torch.long)
        label_fake = torch.zeros(len(predictions_fake), dtype=torch.long)

        loss_D = self.criterion_adv( predictions, torch.cat(label_real, label_fake))
        # TODO : loss_G

        return loss_D


class TextCNN(nn.Module):
    def __init__(self, dim_h, n_filters, filter_sizes, output_dim, dropout): 
        # 원본 코드 상의 output_dim은 1
        super().__init__()
        self.convs = nn.ModuleList([
                                    nn.Conv2d(in_channels = 1,
                                              out_channels = n_filters,
                                              kernel_size = (fs, dim_h)) \
                                    for fs in filter_sizes
                                    ])
        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)
        self.dropout = nn.Dropout(dropout)
    def forward(self, hiddens): 
        #hiddens = [batch_size, hiddens len, dim_h]
        hiddens = hiddens.unsqueeze(1)
        #hiddens = [batch_size, 1, hiddens len, dim_h]

        conved = [F.leaky_relu(conv(embedded)).squeeze(3) for conv in self.convs]
        #conved[n] = [batch size, n_filters, dim_h - filter_sizez[n] + 1]

        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]
        #pooled[n] = [batch size, n_filters]

        cat = self.dropout(torch.cat(pooled, dim =1))
        #cat = [batch size, n_filters * len(filter_sizes)]

        return self.fc(cat)

"""
앞서 작성한 module들을 합쳐 Transfer 모듈을 정의
"""

def adv_loss()

class Transfer(nn.Module):
    def __init__(self, dim_y, dim_z, pad_idx):
        super().__init__()
        # Hyper parameters
        self.dim_y =
        self.dim_z =
        self.batch_size = # ?
        self.max_len = max_seq_len

        self.gamma_ = #temperatureT
        self.lambda_ = #lagrange multiplier
        
        #Embedding layer
        self.embed = # TODO
        
        #Encoder
        self.encoder = Encoder()
        #Generator - unroll and get outputs
        self.generator = Decoder()
        #Discriminator
        self.discriminator_0 = Discriminator()
        self.discriminator_1 = Discriminator()

        self.criterion_rec = nn.CrossEntropyLoss(ignore_index=pad_idx)


    def forward(self, src, src_len, labels):
        embedded_src = self.embed(src)

        # get z from encoder
        hidden_z = self.encoder(src, src_len, labels)

        # Unroll G from initial state get hidden states sequence h_original
        ouputs_ori, hiddens_ori, predictions_ori = self.decoder(labels, 
                                                                z, src, 
                                                                src_len, 
                                                                transfered = False)

        # Unroll G from initial state get hidden states sequence h_transfer(use feed)
        outputs_trans, hiddens_trans, predictions_trans = self.decoder(labels, z, src, src_len, tranfered = True)
        
        predictions_ori = predictions_ori[1:].view(-1,ouputs.size(-1))
        loss_rec = self.loss_rec(predictions_ori[1:],embedded_src[1:].view(-1) )


        # 아래는 원본 코드가 이렇게 되어 있어서... 맨앞에 h0를 붙여준다.
        teach_h = torch.concat((hidden_z.unsqueeze(1), outputs), 1)
        loss_adv0 = discriminator_0()
        loss_adv1 = discriminator_1()

        loss_all = loss_rec - self.labmda_(loss_adv0 + loss_adv1)
        
        return loss_rec, loss_adv1, loss_adv2, loss_all

"""## Training the Model
-
"""

model = Transfer()


for epoch in

import tensorflow as tf

#labels_in = tf.placeholder(tf.float32, [None], # 2 일 수도 있고 3일수도 있는 건가
#            name='labels')
labels_in = tf.constant([1.0, 1.0, 1.0, 1.0, 1.0, 0.0])
labels_in = tf.reshape(labels_in, [-1, 1])
dim_y = 3

def linear(inp, dim_out, reuse=False):
    dim_in = inp.get_shape().as_list()[-1]

    W = tf.compat.v1.get_variable('W', [dim_in, dim_out])
    b = tf.compat.v1.get_variable('b', [dim_out])
    return tf.matmul(inp, W) + b

linear(labels_in, dim_y)

x = tf.constant([1.0, 1.0, 1.0, 1.0, 0.0])
print(x.shape)
tf.expand_dims(x,1).shape

1-labels_in

import torch
import torch.nn as nn
labels_in = torch.tensor([1., 1., 1., 1., 1., 0.])
labels_in = labels_in.unsqueeze(-1)
fc = nn.Linear(1, dim_y)
fc(labels_in)

"""# 질문거리

- [논문] p.6 상단, 같은 t인가? 서로 다른 time step이라면 어디까지? -> Generator line. 30 참고
- 관련해서 seq2seq에서는 <eos>가 나올 때 까지 decoding을 진행해야 하는 건가? -> seq2seq translate 함수를 아직 안만들었다.
- [논문] 마찬가지로 논문 상단에 h\_0 -> h\_1 에서 그냥 그대로 사용 하는게 맞는지? 아니면
$h_1 = Wh_0$ 가 되는건가? 일단 코드는 그대로 사용하는 것으로 짰다
- [코드] style_transfer.py 에서 soft_h_tsf 에는 h0을 안붙이고, teacher forcing 한 애들한테는 붙이는데, 이 차이를 어떻게 이해해야 하나? [line. 90, line.96]
- [의논] 배치를 어떻게 만드는게 낳을지?? 
- [seq2seq 코드] 에서 train할 때 왜 output[1:] 이렇게 시작하는 것인지?
- [코드] discriminator line 162 이하 부분에서 label은 사실 중요하지 않은건가?...
- [pytorch_loss] 모델 안에 loss가 들어가도 괜찮겠지??
- [reconstruction] 부분에서 막혔음
"""

batch_size=4
hidden_size=5
max_time=3
input_data = tf.random.normal([4,max_time,8])

rnn_cell = tf.compat.v1.nn.rnn_cell.BasicRNNCell(hidden_size)

# 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]

# defining initial state
initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)

# 'state' is a tensor of shape [batch_size, cell_state_size]
outputs, state = tf.compat.v1.nn.dynamic_rnn(rnn_cell, input_data,
                                   initial_state=initial_state,
                                   dtype=tf.float32)

outputs.shape

a = torch.rand(3,5, 17)
a.shape

a.view(-1)

